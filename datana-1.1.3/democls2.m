clc
echo on
% keywords: demo
% Here we demonstrate some basic 'pattern recognition' problems:
%
%  - the identifation of subgroups or 'patterns' in a data set
%  - how to give a computational representation for a subgroup
%  - the classification of observations: do they belong to a certain
%    subgroup ?
%
% In this demonstration we use two sets of simulated data generated by
% multidimensional Gaussian distributions. We generate two sets of
% points in a 10-dimensional space, both consisting of 50 points
% situated in a 'cigar', an ellipsoid with a given center point and
% semiaxes. In more practical terms, this would correspond to having
% 100 observations, each with 10 variables.
%
% We first generate the data using the function MGAUSS (for details, see
% the help of  MGAUSS and the file 'democlas.m')
%
% The 'observations' are simulated by a random number generator. So the
% results will be different in each run of this demonstration.

echo off
 m   = 50;        % the number of observations
 n   = 10;        % the number of variables
 ndim = 3;        % the number of dimensions used in the classification

 x01 = ones(n,1);             % the center point of group 1
 x02 = -7*x01;                % the center point of group 2
 d   = [100 ones(1,n-3) 1 1]; % the lengths of the semiaxes of the
                              % ellipsoids (same for both groups)

 x1  = ones(n,1); ll = length(x1); lll = round(ll/2)+1:ll;
 x2  = x1; x2(lll) = -x2(lll); x2(2)=0;
                              % the directions of the lengthiest axes

 y1 = mgauss(x01,m,x1,d);     % generate the points
 y2 = mgauss(x02,m,x2,d);

 y  = [y1; y2];               % the combined data set

pause, clc %strike any key to continue ...
echo on
%
% Next we have a look at our data by the principal component analysis,
% PCA. Using the function PCA we compute the scores 't' ,loadings
% 'p' and the coefficient of determination 'r2' by the command
%
% [t,p,r2] = pca(center(y));
%
% and plot the scores, the 2-dimensional projections on the 1. and 2.
% principal components of the originally 10-dimensional observations,
% by
%
% plotpca(t,1,2,1:100)
%

pause, clc %strike any key to continue ...
echo off

  [t,p,r2] = pca(center(y));
  nobs = 1:2*m;
  plotpca(t,1,2,nobs);
  title('a PCA score plot of the observations')
  pause
 
echo on
% Clearly there seems to be two different subgroups in the data. Does
% the 2-dimensional plots of 10-dimensional observations contain enough
% information for such a conclusion ? One way of judgning this is to
% check the vector 'r2', which tells which part of the total variation of
% the data the principal components (cumulatively) explain. In this case

 r2'

% i.e., the first two components give over 96 procent of the variation
% of the data (usually, the data is generated randomly for each run!)
% So a small number of principal components should reveal enough
% information for the classification.

  pause, clc %strike any key to continue ...
%
% We skip further details on how to select the points which belong to
% each of the groups and take it for granted that the first half of the
% data set belongs to one and the rest to the other group.
%
% In any case, the 2-dimensional projections give slightly overlapping
% point sets in the score plot. For the representation and classification
% of the observations we thus better take more dimensions.
%
% Let us  compute 3-dimensional representations for the two subsets.
% That is, we compute the center points and 3 largest semiaxes for
% the 'cigars' (ellipsoids) which approximate the 'swarm' of observations.
%
% The function REPRES computes the center point and the required number
% of principal axes for a given data set. We use it for both of the
% data sets.
%

 pause, clc % strike any key to continue ...

 [xe1,p1,s1] = repres(y1,ndim);
 [xe2,p2,s2] = repres(y2,ndim);

% Next, compute the classification of all the observations.
%
% We first check which of the observations would belong to the first group:

 [ind1,xp1,level1] = classi(y,xe1,p1,s1);

  echo off
  plotpca(xp1,1,2,ind1);
  title('the points classified to be (1) and not to be (0) in group 1')
  pause

  echo on
% and the same for the second group:

 [ind2,xp2,level2] = classi(y,xe2,p2,s2);

  echo off
  plotpca(xp2,1,2,ind2);
  title('the points classified to be (1) and not to be (0) in group 2')
  pause, clc

  echo on
% Finally, we take new observations from the same 'sources', i.e.,
% we generate two new sets of points with the same distributions
% as those with 'group 1' and 'group 2'. Based on the earlier
% calibrations, classifications are computed for the new points:
% do they belong to the first or second group.

% Generate:

 y1 = mgauss(x01,m,x1,d);
 y2 = mgauss(x02,m,x2,d);
 y  = [y1; y2];
 
% The first group:


 [ind3,xp3,level1] = classi(y,xe1,p1,s1);

 pause, clc % strike any key to continue ...

  echo off
  plotpca(xp3,1,2,ind3);
  title('new points classified to be (1) and not to be (0) in group 1')
  pause

  echo on
% and the second one:

 [ind4,xp4,level2] = classi(y,xe2,p2,s2);

  echo off
  plotpca(xp4,1,2,ind4);
  title('new points classified to be (1) and not to be (0) in group 2')
  pause

  i=1:m; ii=m+1:2*m;
  i1(1) = length(find(ind1(i)==0)); i2(1) = length(find(ind1(ii)==1));
  i1(2) = length(find(ind2(i)==1)); i2(2) = length(find(ind2(ii)==0));
  i1(3) = length(find(ind3(i)==0)); i2(3) = length(find(ind3(ii)==1));
  i1(4) = length(find(ind4(i)==1)); i2(4) = length(find(ind4(ii)==0));
  i1 = i1'; i2 = i2';
  i1 = i1/m*100;  i2 = i2/m*100;

 clc
 echo on
% Two kind of classification errors may occur here:
%
% - A point belongs to a group, but is classified not to belong,
%   an error of the first kind
% - A point does not belong to a group, but is classified to belong,
%   an error of the second kind
%
% In the function CLASSI, we have here used the default 95 % level
% for the error of the first kind: in average, 5 % of correct points
% are classified as incorrect. With the sample size 50 used here, the
% real error may be considerably different. With larger sample sizes,
% 500-1000, say, the error tends to be closer to the expected one. Using
% larger values for the parameter 'level' in CLASSI you may decrease
% errors of the first kind - and increase errors of the second kind.
%
% Errors of the second kind are inevitable, if the 'cigars' of observations
% overlap. In this example, you may eliminate errors of second kind by
% moving the ellipsoids further away from each other (edit democlas.m)
%
% With small  (10-20) sample sizes of the calibration sets the PCA
% representations for the subgroups may be rather random. Consequently,
% classifications of new observations may be erratic.
  pause, clc % strike any key to continue ...
 echo off

disp(' Statistics of the misclassifications in each case:')
 disp(' ')
%
disp(' procent of failures for calibration data, classification to group 1:')

        [i1(1)  i2(1)]
disp('group1         group2  ')
disp('err of 1.kind  err of 2.kind')
disp(' ')
disp(' procent of failures for calibration data, classification to group 2:')
        [i1(2)  i2(2)]
disp('group1         group2  ')
disp('err of 2.kind  err of 1.kind')

pause, clc

disp(' ')
disp(' ')
disp(' procent of failures for new data, classification to group 1:')
        [i1(3)  i2(3)]
disp('group1         group2  ')
disp('err of 1.kind  err of 2.kind')
disp(' ')

disp(' procent of failures for new data, classification to group 2:')
        [i1(4)  i2(4)]
disp('group1         group2  ')
disp('err of 2.kind  err of 1.kind')

